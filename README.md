# ğŸŒ¦ Weather Data Pipeline - SÃ£o Paulo

End-to-end ETL pipeline that extracts real-time weather data for SÃ£o
Paulo using the OpenWeather API, transforms it using Pandas, and loads
it into PostgreSQL. The pipeline is orchestrated using Apache Airflow
and fully containerized with Docker.

------------------------------------------------------------------------

## ğŸ“Œ Project Overview

This project simulates a production-ready data engineering workflow:

-   Extract weather data from external API
-   Transform nested JSON into structured tabular format
-   Load processed data into PostgreSQL
-   Orchestrate tasks using Airflow DAG
-   Containerized environment with Docker

------------------------------------------------------------------------

## ğŸ— Architecture

OpenWeather API\
â¬‡\
Airflow (ETL Orchestration)\
â¬‡\
Pandas Transformations\
â¬‡\
PostgreSQL Database

------------------------------------------------------------------------

## ğŸ›  Tech Stack

-   Python 3.12
-   Pandas
-   Apache Airflow
-   Docker & Docker Compose
-   PostgreSQL
-   python-dotenv
-   uv (Python package manager)

------------------------------------------------------------------------

## ğŸ“‚ Project Structure

    weather_data_pipeline/
    â”‚
    â”œâ”€â”€ dags/
    â”‚   â””â”€â”€ weather_dag.py
    â”‚
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ extract_data.py
    â”‚   â”œâ”€â”€ transform_data.py
    â”‚   â””â”€â”€ load_data.py
    â”‚
    â”œâ”€â”€ data/
    â”œâ”€â”€ config/
    â”‚   â””â”€â”€ .env
    â”‚
    â”œâ”€â”€ docker-compose.yaml
    â”œâ”€â”€ pyproject.toml
    â””â”€â”€ README.md

------------------------------------------------------------------------

## âš™ï¸ How It Works

### 1ï¸âƒ£ Extract

Fetches weather data from OpenWeather API.

### 2ï¸âƒ£ Transform

-   Flattens nested JSON
-   Normalizes weather fields
-   Cleans column names
-   Stores intermediate result as Parquet

### 3ï¸âƒ£ Load

-   Reads processed data
-   Inserts into PostgreSQL table `sp_weather`

------------------------------------------------------------------------

## ğŸš€ Running the Project

### 1. Set environment variable

Create a `.env` file inside `config/`:

    api_key=YOUR_OPENWEATHER_API_KEY

### 2. Start containers

    docker compose up -d

### 3. Access Airflow UI

    http://localhost:8080

Enable DAG:

    weather_pipeline

------------------------------------------------------------------------

## ğŸ“Š Example Output Columns

  Column                Description
  --------------------- ---------------------------
  temp                  Current temperature
  humidity              Air humidity
  wind_speed            Wind speed
  weather_main          Weather category
  weather_description   Detailed condition
  datetime              Data extraction timestamp

------------------------------------------------------------------------

## ğŸ¯ Learning Goals

This project demonstrates:

-   API integration
-   JSON normalization
-   Data transformation with Pandas
-   Airflow DAG development
-   Docker containerization
-   ETL best practices

------------------------------------------------------------------------

## ğŸ”® Future Improvements

-   Add data quality validation
-   Implement incremental loads
-   Store historical weather data
-   Deploy to cloud environment (GCP)
-   Add monitoring and alerts

------------------------------------------------------------------------

## ğŸ‘©â€ğŸ’» Author

Adria Freitas
